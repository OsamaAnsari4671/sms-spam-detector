import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import nltk
import warnings
warnings.filterwarnings('ignore')





df = pd.read_csv('spam.csv', encoding='latin-1') #import dataset


df.head()


df.info()


df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True) # removing columns that provides no value


df.head()


df.isnull().sum() # check for null values. 


df.rename(columns={'v1':'target', 'v2':'text'}, inplace=True) # renaming columns


df.head()


df['target'].unique() 


df['target'] = df['target'].replace({'ham':0, 'spam':1}) # encoding target variables


df.head()


df['target'].value_counts() 


df.duplicated().sum() # checking for duplicates


df.drop_duplicates(inplace=True, keep='first') # dropping duplicates





# No. of spams and not spams
plt.pie(x=df['target'].value_counts(), labels=['not spam', 'spam'], autopct='%0.2f')
plt.show()








df['num_chars'] = df['text'].apply(lambda x: len(x)) # getting the number of characters in each text
df['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x))) # number of words used
df['num_sentences'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x))) # number of sentences used


print('Descriptive analysis for not spam text:\n', df[df['target']==0].describe().T ,'\n')
print('='*100)
print('Descriptive analysis for spam text:\n', df[df['target']==1].describe().T)





# No. of characters in spam VS non-spam sms
sns.histplot(df[df['target']==0], x='num_chars', label='Not Spam')
sns.histplot(df[df['target']==1], x='num_chars', label='Spam')
plt.legend()
plt.show()


# No. of words in spam VS non-spam sms
sns.histplot(df[df['target']==0], x='num_words', label='Not Spam')
sns.histplot(df[df['target']==1], x='num_words', label='Spam')
plt.legend()
plt.show()


# No. of sentences in spam VS non-spam sms
sns.histplot(df[df['target']==0], x='num_sentences', label='Not Spam')
sns.histplot(df[df['target']==1], x='num_sentences', label='Spam')
plt.legend()
plt.show()














from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer


nltk.download('stopwords')
stop_words = list(stopwords.words('english'))
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()


# create a function to preprocess the text

def preprocess_text(text):
    words = nltk.word_tokenize(text.lower())
    filtered_words = [stemmer.stem(word) for word in words if word.isalnum() and word not in stop_words]
    return ' '.join(filtered_words)


df['clean_text'] = df['text'].apply(preprocess_text) # adding a column to store clean processed texts


df.head()





from wordcloud import WordCloud
wc = WordCloud(width=600, height=400, min_font_size=8, background_color='white')


not_spam_wc = wc.generate(df[df['target']==0]['clean_text'].str.cat(sep=" "))
plt.figure(figsize=(10,6))
plt.imshow(not_spam_wc)
plt.tight_layout()
plt.show()


spam_wc = wc.generate(df[df['target']==1]['clean_text'].str.cat(sep=' '))
plt.figure(figsize=(10,6))
plt.imshow(not_spam_wc)
plt.tight_layout()
plt.show()


tfidf = TfidfVectorizer(max_features=3000)  
x = tfidf.fit_transform(df['clean_text']).toarray()
y = df['target']


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)


from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix, f1_score


# model training 
models = {'Naive Bayes': MultinomialNB(),
          'Logistic Regression': LogisticRegression(),
          'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
          'Support Vector machine': SVC(),
          'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
              
}


for name, model in models.items():
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    #report = classification_report(y_test, y_pred)
    #cm = confusion_matrix(y_test, y_pred)

    print(f'Model Name: {name}')
    print(f'Accuracy Score: {accuracy:.2f}')
    print(f'Precision Score: {precision:.2f}')
    print(f'F1 Score: {f1:.2f}')
    #print(f'Classification Report:\n', report)
    #print(f'Confusion Matrix:\n', cm)
    print('='*50)
    





from sklearn.model_selection import RandomizedSearchCV


alpha_values = [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0]
param_grid = {'alpha': alpha_values}


random_grid = RandomizedSearchCV(estimator=MultinomialNB(), param_distributions=param_grid, cv=5, n_iter=20 ,scoring='accuracy')


random_grid.fit(x_train, y_train)


random_grid.best_params_ 


MNB = MultinomialNB(alpha=0.1)


MNB.fit(x_train, y_train)
y_pred_mnb = MNB.predict(x_test)


print(f'Accuracy Score: {accuracy_score(y_test, y_pred_mnb)}')
print(f'Precision Score: {precision_score(y_test, y_pred_mnb)}')





best_MNB = MultinomialNB(alpha=1.0)


best_MNB.fit(x_train, y_train)


new_data = 'Win 1OLakh/- cash on Zupee va1.in/N3-zp'


# Preprocess the new text string
preprocessed_text = preprocess_text(new_data)

# Transform the preprocessed text into a TF-IDF vector
new_text_vector = tfidf.transform([preprocessed_text])

# Use the trained Multinomial Naive Bayes model to make a prediction
prediction = best_MNB.predict(new_text_vector)

if prediction == 0:
    print('not spam')
else:
    print('spam')



import pickle
pickle.dump(preprocess_text, open('preprocess_text.pkl', 'wb'))
pickle.dump(tfidf, open('vectorizer.pkl','wb'))
pickle.dump(best_MNB, open('model.pkl','wb'))


!pip freeze > requirements.txt






